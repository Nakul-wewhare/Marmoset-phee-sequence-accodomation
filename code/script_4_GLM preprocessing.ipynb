{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bf4086",
   "metadata": {},
   "source": [
    "### in this script we prepare the data for the glm model, were we calculate distance per stage per pair, between all session combinations for whole, only partner and only non partner data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59aeeae",
   "metadata": {},
   "source": [
    "we first process the sequnce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WHOLE (1619 rows) ===\n",
      "saved c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\glm data\\glm_data_seq_whole.csv  |  628 comparisons\n",
      "\n",
      "=== PARTNER (476 rows) ===\n",
      "saved c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\glm data\\glm_data_seq_partner.csv  |  248 comparisons\n",
      "\n",
      "=== NON_PARTNER (1143 rows) ===\n",
      "saved c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\glm data\\glm_data_seq_non_partner.csv  |  500 comparisons\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 0)  Imports & configuration  \n",
    "# ---------------------------------------------------------------------------\n",
    "import os, itertools, numpy as np, pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform     \n",
    "from scipy.spatial import distance as ssd\n",
    "\n",
    "BASE_DIR      = Path.cwd().parent\n",
    "SEQ_DATA_DIR  = BASE_DIR / \"seqeunce data\"\n",
    "\n",
    "CSV_PATH      = SEQ_DATA_DIR / \"Processed_seq_data.csv\"     # master CSV\n",
    "\n",
    "PAIRS = {\"Tabor\":\"Lola\",\"Odin\":\"Nougatti\",\"Wuschel\":\"Olympia\"} | \\\n",
    "        {\"Lola\":\"Tabor\",\"Nougatti\":\"Odin\",\"Olympia\":\"Wuschel\"}\n",
    "\n",
    "MIN_SEQ_PER_LIST = 5\n",
    "NGRAM_N, ELEMENT_OF_INT, MAX_REPEAT_LEN = 2, \"A\", 5\n",
    "OUT_BASENAME = \"glm_data_seq_{}.csv\"            # csv of all distances\n",
    "\n",
    "OUTPUT_DIR    = BASE_DIR / \"glm data\"                       # ✦ NEW\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 1)  Metric functions  (identical to originals)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def smith_waterman(sequence1, sequence2, match_score=1, mismatch_score=-2, gap_penalty=-2):\n",
    "    m, n = len(sequence1), len(sequence2)\n",
    "    score_matrix = [[0]*(n+1) for _ in range(m+1)]\n",
    "    traceback_matrix = [['']*(n+1) for _ in range(m+1)]\n",
    "    max_score, max_pos = 0, (0, 0)\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            match  = score_matrix[i-1][j-1] + (match_score if sequence1[i-1]==sequence2[j-1] else mismatch_score)\n",
    "            delete = score_matrix[i-1][j]   + gap_penalty\n",
    "            insert = score_matrix[i][j-1]   + gap_penalty\n",
    "            score_matrix[i][j] = max(0, match, delete, insert)\n",
    "\n",
    "            if score_matrix[i][j] == match:   traceback_matrix[i][j] = '\\\\'\n",
    "            elif score_matrix[i][j] == delete: traceback_matrix[i][j] = '^'\n",
    "            elif score_matrix[i][j] == insert: traceback_matrix[i][j] = '<'\n",
    "\n",
    "            if score_matrix[i][j] > max_score:\n",
    "                max_score, max_pos = score_matrix[i][j], (i, j)\n",
    "\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def distance_local_alignment_internal(seqs1, seqs2):\n",
    "    scores = []\n",
    "    for s1 in seqs1:\n",
    "        best = float('-inf')\n",
    "        for s2 in seqs2:\n",
    "            sc = smith_waterman(s1, s2) / np.log(len(s2))\n",
    "            best = max(best, sc)\n",
    "        scores.append(best)\n",
    "    return -np.sqrt(np.mean(np.square(scores)))\n",
    "\n",
    "\n",
    "def distance_local_alignment(seqs1, seqs2):\n",
    "    return (distance_local_alignment_internal(seqs1, seqs2) +\n",
    "            distance_local_alignment_internal(seqs2, seqs1)) / 2\n",
    "\n",
    "\n",
    "def calculate_transition_matrix_(sequences, alphabet):\n",
    "    tm = pd.DataFrame(0, index=list(alphabet), columns=list(alphabet))\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq)-1):\n",
    "            tm.at[seq[i], seq[i+1]] += 1\n",
    "    return tm.div(tm.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "def distance_transition_matrix(seqs1, seqs2, alphabet):\n",
    "    m1 = calculate_transition_matrix_(seqs1, alphabet).to_numpy().flatten()\n",
    "    m2 = calculate_transition_matrix_(seqs2, alphabet).to_numpy().flatten()\n",
    "    return np.linalg.norm(np.nan_to_num(m1) - np.nan_to_num(m2))\n",
    "\n",
    "\n",
    "def calculate_ngram_prob_distribution(sequences, n, all_unique_ngrams):\n",
    "    counts = {ng: 0 for ng in all_unique_ngrams}\n",
    "    for seq in sequences:\n",
    "        ngrams = [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)]\n",
    "        for ng in ngrams: counts[ng] += 1\n",
    "    tot = sum(counts.values()) + 1e-3\n",
    "    return {ng: c/tot for ng, c in counts.items()}\n",
    "\n",
    "\n",
    "def distance_n_gram(seqs1, seqs2, n, alphabet):\n",
    "    all_ngrams = [tuple(''.join(t)) for t in itertools.product(alphabet, repeat=n)]\n",
    "    v1 = np.array([calculate_ngram_prob_distribution([s for s in seqs1 if len(s)>=n], n, all_ngrams)[ng] for ng in all_ngrams])\n",
    "    v2 = np.array([calculate_ngram_prob_distribution([s for s in seqs2 if len(s)>=n], n, all_ngrams)[ng] for ng in all_ngrams])\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "\n",
    "def calculate_repeat_length_prob_distribution_specific(sequences, max_len, element):\n",
    "    counts = {l: 0 for l in range(2, max_len+1)}\n",
    "    for seq in sequences:\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            cnt = 1\n",
    "            while i+1 < len(seq) and seq[i]==seq[i+1]:\n",
    "                i += 1; cnt += 1\n",
    "            if 1 < cnt <= max_len and seq[i] in element: counts[cnt] += 1\n",
    "            i += 1\n",
    "    tot = sum(counts.values()) + 1e-3\n",
    "    return {l: c/tot for l, c in counts.items()}\n",
    "\n",
    "\n",
    "def distance_repeat_specific(seqs1, seqs2, element, max_len=5):\n",
    "    p1 = calculate_repeat_length_prob_distribution_specific(seqs1, max_len, element)\n",
    "    p2 = calculate_repeat_length_prob_distribution_specific(seqs2, max_len, element)\n",
    "    lens = sorted(set(p1)|set(p2))\n",
    "    v1 = np.array([p1.get(l,0) for l in lens])\n",
    "    v2 = np.array([p2.get(l,0) for l in lens])\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 2)  Helper: build one distance table  (only change: session_number)\n",
    "# ---------------------------------------------------------------------------\n",
    "def build_distance_table(df_subset: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = (\n",
    "        df_subset\n",
    "        .groupby(['focal ID', 'stage', 'session_number'])['true_sequence']\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "    grouped = {k:v for k,v in grouped.items() if len(v) >= MIN_SEQ_PER_LIST}\n",
    "    alphabet = set(''.join(df_subset['true_sequence']))\n",
    "    rows = []\n",
    "\n",
    "    for ind1, ind2 in [('Tabor','Lola'), ('Odin','Nougatti'), ('Wuschel','Olympia')]:\n",
    "        for stage in ['before','after']:\n",
    "            sess1 = [s for (_,st,s) in grouped if _==ind1 and st==stage]\n",
    "            sess2 = [s for (_,st,s) in grouped if _==ind2 and st==stage]\n",
    "            for s1 in sess1:\n",
    "                seq1 = grouped[(ind1,stage,s1)]\n",
    "                for s2 in sess2:\n",
    "                    seq2 = grouped[(ind2,stage,s2)]\n",
    "                    rows.extend([\n",
    "                        dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                             session_focal=s1, session_partner=s2,\n",
    "                             metric='local_alignment',\n",
    "                             distance=distance_local_alignment(seq1,seq2)),\n",
    "                        dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                             session_focal=s1, session_partner=s2,\n",
    "                             metric='transition_matrix',\n",
    "                             distance=distance_transition_matrix(seq1,seq2,alphabet)),\n",
    "                        dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                             session_focal=s1, session_partner=s2,\n",
    "                             metric='bigram',\n",
    "                             distance=distance_n_gram(seq1,seq2,NGRAM_N,alphabet)),\n",
    "                        dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                             session_focal=s1, session_partner=s2,\n",
    "                             metric='repeat_A_len',\n",
    "                             distance=distance_repeat_specific(seq1,seq2,ELEMENT_OF_INT,MAX_REPEAT_LEN)),\n",
    "                    ])\n",
    "    df = pd.DataFrame(rows)\n",
    "    # ── normalisations ──────────────────────────────────────────────\n",
    "    df['z_distance'] = (\n",
    "        df.groupby('metric')['distance']\n",
    "        .transform(lambda x: (x - x.mean()) / x.std(ddof=0)\n",
    "                            if x.std(ddof=0) else 0.0)\n",
    "    )\n",
    "\n",
    "    df['distance_01'] = (\n",
    "        df.groupby('metric')['distance']\n",
    "        .transform(lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "                            if (x.max() - x.min()) else 0.0)\n",
    "    )  # min–max scaling 0‥1\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 4)  Run pipeline for each subset, save distance & summary tables\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "seq_data = pd.read_csv(CSV_PATH)          # typo fixed (was FILE_PATH)\n",
    "\n",
    "subsets = {\n",
    "    \"whole\":      seq_data,\n",
    "    \"partner\":     seq_data[seq_data['paired_status']==\"partner\"],\n",
    "    \"non_partner\": seq_data[seq_data['paired_status']==\"non-partner\"],\n",
    "}\n",
    "\n",
    "for label, df_sub in subsets.items():\n",
    "    print(f\"\\n=== {label.upper()} ({len(df_sub)} rows) ===\")\n",
    "    dist_tbl = build_distance_table(df_sub)\n",
    "\n",
    "    outfile = OUTPUT_DIR / OUT_BASENAME.format(label)       # \n",
    "    dist_tbl.to_csv(outfile, index=False)                   # \n",
    "    print(f\"saved {outfile}  |  {len(dist_tbl)} comparisons\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecd20d",
   "metadata": {},
   "source": [
    "we now process the spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0fd897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WHOLE  (3904 calls) ===\n",
      "  → DTW will be computed for 182 session pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved glm_data_spec_whole.csv | 546 comparisons\n",
      "\n",
      "=== PAIRED  (1013 calls) ===\n",
      "  → DTW will be computed for 76 session pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved glm_data_spec_paired.csv | 228 comparisons\n",
      "\n",
      "=== NON_PAIRED  (2891 calls) ===\n",
      "  → DTW will be computed for 143 session pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved glm_data_spec_non_paired.csv | 429 comparisons\n",
      "✅  Pipeline finished. Output files are in: C:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\glm data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# Spectral Convergence Pipeline – run in VS Code or Jupyter\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mirrors the syntactic‑distance workflow but uses spectral features:\n",
    "#   • Euclidean distance on 5 PCs of traditional acoustics\n",
    "#   • Euclidean distance on 5 PCs of MFCCs (columns 1‑132)\n",
    "#   • DTW alignment‑cost of 4–12 kHz spectrograms\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# Imports & basic configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "import os, itertools, random, pathlib\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, spatial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm                      # <‑‑ progress bars\n",
    "\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# Paths & constants  —— tweak here if directories move\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR      = Path.cwd().parent\n",
    "SPEC_DATA_DIR  = BASE_DIR / \"spectral data\"\n",
    "\n",
    "CSV_PATH      = SPEC_DATA_DIR / \"Processed_spec_data.csv\" \n",
    "df = pd.read_csv(CSV_PATH )\n",
    "\n",
    "# pick a sensible base directory whether running as notebook or script\n",
    "                        # running in Jupyter\n",
    "\n",
    "OUTPUT_DIR   = BASE_DIR / \"glm data\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_BASENAME = OUTPUT_DIR / \"glm_data_spec_{}.csv\"\n",
    "\n",
    "# partner mapping (bidirectional)\n",
    "PAIRS = {\"Tabor\":\"Lola\", \"Odin\":\"Nougatti\", \"Wuschel\":\"Olympia\"}\n",
    "PAIRS |= {v: k for k, v in PAIRS.items()}\n",
    "\n",
    "# analysis parameters\n",
    "MIN_CALLS_PER_LIST = 5\n",
    "DTW_BAND_LO        = 4_000     # Hz\n",
    "DTW_BAND_HI        = 12_000    # Hz\n",
    "SHOW_PROGRESS      = True      # toggle tqdm progress bars\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 2)  Distance‑metric helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# put this near the top, after imports\n",
    "def _count_session_pairs(grouped, ind1, ind2, stage):\n",
    "    sess1 = [k[2] for k in grouped if k[0] == ind1 and k[1] == stage]\n",
    "    sess2 = [k[2] for k in grouped if k[0] == ind2 and k[1] == stage]\n",
    "    return len(sess1) * len(sess2)\n",
    "\n",
    "def dist_pc_pairwise(list1: pd.DataFrame, list2: pd.DataFrame, pc_cols):\n",
    "    \"\"\"Mean pairwise Euclidean distance between two call lists on the given PCs.\"\"\"\n",
    "    dists = spatial.distance.cdist(list1[pc_cols], list2[pc_cols], metric=\"euclidean\")\n",
    "    return float(dists.mean())\n",
    "\n",
    "\n",
    "# -------  DTW helpers (spectrogram → alignment cost)  ------------------------\n",
    "\n",
    "\n",
    "def generate_spectrogram(audio_file_path, pad_duration=0):\n",
    "    \"\"\"Return dB‑scaled spectrogram (freq × time) and its sample rate.\"\"\"\n",
    "    audio = AudioSegment.from_wav(str(audio_file_path))\n",
    "    y = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "    sr = audio.frame_rate\n",
    "    if audio.channels == 2:\n",
    "        y = y.reshape((-1, 2)).mean(axis=1)              # stereo → mono\n",
    "\n",
    "    # symmetric zero‑pad if shorter than target\n",
    "    target_len = int(pad_duration * sr)\n",
    "    if len(y) < target_len:\n",
    "        pad = target_len - len(y)\n",
    "        y = np.pad(y, (pad // 2, pad - pad // 2), mode=\"constant\")\n",
    "\n",
    "    N_FFT      = 2048\n",
    "    HOP_LENGTH = N_FFT // 4\n",
    "    WIN_LENGTH = N_FFT\n",
    "    D = librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, window=\"hann\")\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    return S_db, sr\n",
    "\n",
    "\n",
    "def crop_band(S_db, sr, lo=DTW_BAND_LO, hi=DTW_BAND_HI, n_fft=2048):\n",
    "    \"\"\"Keep only rows within [lo, hi] Hz.\"\"\"\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    band   = (freqs >= lo) & (freqs <= hi)\n",
    "    return S_db[band, :]\n",
    "\n",
    "\n",
    "def compute_alignment_cost(S1, S2):\n",
    "    \"\"\"DTW alignment cost (cosine distance) normalised by total frames.\"\"\"\n",
    "    dist, _ = fastdtw(S1.T, S2.T, dist=cosine)\n",
    "    return dist / (S1.shape[1] + S2.shape[1])\n",
    "\n",
    "\n",
    "# cache spectrograms so we don’t re‑load the same file a hundred times\n",
    "@lru_cache(maxsize=2048)\n",
    "def _spectro_cache(path: str):\n",
    "    S_db, sr = generate_spectrogram(path, pad_duration=0.5)\n",
    "    return crop_band(S_db, sr)\n",
    "\n",
    "\n",
    "def dist_dtw_pairwise(list1: pd.DataFrame, list2: pd.DataFrame):\n",
    "    \"\"\"Mean DTW alignment‑cost between every call in list1 × list2, with a progress bar.\"\"\"\n",
    "    costs = []\n",
    "    n1, n2 = len(list1), len(list2)\n",
    "    total  = n1 * n2\n",
    "\n",
    "    iterator1 = list1[\"call_audio_path\"].tolist()\n",
    "    iterator2 = list2[\"call_audio_path\"].tolist()\n",
    "\n",
    "    use_bar = SHOW_PROGRESS and total > 1\n",
    "    pbar = tqdm(total=total, desc=\"DTW pairwise\", unit=\"align\", leave=False) if use_bar else None\n",
    "\n",
    "    try:\n",
    "        for p1 in iterator1:\n",
    "            S1 = _spectro_cache(p1)\n",
    "            for p2 in iterator2:\n",
    "                S2 = _spectro_cache(p2)\n",
    "                costs.append(compute_alignment_cost(S1, S2))\n",
    "                if pbar is not None:\n",
    "                    pbar.update(1)\n",
    "    finally:\n",
    "        if pbar is not None:\n",
    "            pbar.close()\n",
    "\n",
    "    return float(np.mean(costs))\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 4)  Build distance table for a given subset (whole / paired / non‑paired)\n",
    "# ---------------------------------------------------------------------------\n",
    "from contextlib import nullcontext     # add with other imports\n",
    "\n",
    "def build_distance_table_spectral(df_subset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # --- group calls by (focal, stage, session) -----------------------------\n",
    "    grouped = {\n",
    "        k: g for k, g in df_subset.groupby(\n",
    "            [\"focal ID\", \"stage\", \"session_number\"]\n",
    "        ) if len(g) >= MIN_CALLS_PER_LIST\n",
    "    }\n",
    "\n",
    "    # ---------- NEW: pre-compute how many session pairs will need DTW -------\n",
    "    total_pairs = sum(\n",
    "        _count_session_pairs(grouped, ind1, ind2, stage)\n",
    "        for ind1, ind2 in [(\"Tabor\", \"Lola\"),\n",
    "                           (\"Odin\",  \"Nougatti\"),\n",
    "                           (\"Wuschel\", \"Olympia\")]\n",
    "        for stage in [\"before\", \"after\"]\n",
    "    )\n",
    "    if SHOW_PROGRESS:\n",
    "        print(f\"  → DTW will be computed for {total_pairs} session pairs\")\n",
    "\n",
    "    outer_bar = tqdm(total=total_pairs, desc=\"DTW session-pairs\",\n",
    "                     unit=\"pair\", leave=False) if SHOW_PROGRESS else nullcontext()\n",
    "\n",
    "    rows = []\n",
    "    try:\n",
    "        for ind1, ind2 in [(\"Tabor\", \"Lola\"),\n",
    "                           (\"Odin\",  \"Nougatti\"),\n",
    "                           (\"Wuschel\", \"Olympia\")]:\n",
    "            for stage in [\"before\", \"after\"]:\n",
    "                sess1 = [k[2] for k in grouped if k[0] == ind1 and k[1] == stage]\n",
    "                sess2 = [k[2] for k in grouped if k[0] == ind2 and k[1] == stage]\n",
    "                for s1 in sess1:\n",
    "                    L1 = grouped[(ind1, stage, s1)]\n",
    "                    for s2 in sess2:\n",
    "                        L2 = grouped[(ind2, stage, s2)]\n",
    "                        sess_pair = f\"({s1},{s2})\"\n",
    "\n",
    "                        # ── Euclidean distances on PCs ─────────────────────\n",
    "                        rows += [\n",
    "                            dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                                 session_focal=s1, session_partner=s2,\n",
    "                                 sess_pair=sess_pair,\n",
    "                                 metric=\"T_PC_pairwise\",\n",
    "                                 distance=dist_pc_pairwise(L1, L2,\n",
    "                                                           [f\"trad_PC{i}\" for i in range(1, 6)])),\n",
    "                            dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                                 session_focal=s1, session_partner=s2,\n",
    "                                 sess_pair=sess_pair,\n",
    "                                 metric=\"M_PC_pairwise\",\n",
    "                                 distance=dist_pc_pairwise(L1, L2,\n",
    "                                                           [f\"mfcc_PC{i}\" for i in range(1, 6)])),\n",
    "                        ]\n",
    "\n",
    "                        # ── DTW – slow part, keep last to benefit from bar ─\n",
    "                        rows.append(\n",
    "                            dict(pair=f\"{ind1}-{ind2}\", stage=stage,\n",
    "                                 session_focal=s1, session_partner=s2,\n",
    "                                 sess_pair=sess_pair,\n",
    "                                 metric=\"spectro_DTW\",\n",
    "                                 distance=dist_dtw_pairwise(L1, L2))\n",
    "                        )\n",
    "\n",
    "                        if SHOW_PROGRESS:\n",
    "                            outer_bar.update(1)\n",
    "    finally:\n",
    "        if SHOW_PROGRESS:\n",
    "            outer_bar.close()\n",
    "\n",
    "    # ---------- normalise ---------------------------------------------------\n",
    "    dist_df = pd.DataFrame(rows)\n",
    "    dist_df[\"z_distance\"] = dist_df.groupby(\"metric\")[\"distance\"] \\\n",
    "        .transform(lambda x: (x - x.mean()) / x.std(ddof=0) if x.std(ddof=0) else 0.0)\n",
    "    dist_df[\"distance_01\"] = dist_df.groupby(\"metric\")[\"distance\"] \\\n",
    "        .transform(lambda x: (x - x.min()) / (x.max() - x.min()) if (x.max() - x.min()) else 0.0)\n",
    "    return dist_df\n",
    "\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 5)  Run pipeline over (whole / paired / non‑paired) subsets\n",
    "# ---------------------------------------------------------------------------\n",
    "subsets = {\n",
    "    \"whole\": df,\n",
    "    \"paired\": df[df[\"paired_status\"] == \"partner\"],\n",
    "    \"non_paired\": df[df[\"paired_status\"] == \"non-partner\"],\n",
    "}\n",
    "\n",
    "for tag, subdf in subsets.items():\n",
    "    print(f\"\\n=== {tag.upper()}  ({len(subdf)} calls) ===\")\n",
    "\n",
    "    # -- distance table ------------------------------------------------------\n",
    "    dist_tbl = build_distance_table_spectral(subdf)\n",
    "    dist_csv = OUT_BASENAME.name.format(tag)\n",
    "    dist_tbl.to_csv(OUTPUT_DIR / dist_csv, index=False)\n",
    "    print(\"saved\", dist_csv, \"|\", len(dist_tbl), \"comparisons\")\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "print(\"\\u2705  Pipeline finished. Output files are in:\", OUTPUT_DIR.resolve())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
