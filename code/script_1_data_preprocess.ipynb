{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this script does basic data cleaning and precrossing of the spectral and seqeunce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Project path configuration (Jupyter-only) ────────────────────────────────\n",
    "from pathlib import Path\n",
    "\n",
    "# Current working dir = “…/2025/code/”\n",
    "BASE_DIR  = Path.cwd().parent              # → “…/2025”\n",
    "CODE_DIR  = BASE_DIR / \"code\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we modify the log file, to add session numbers from the dates of the unique recordings, we know each combination of animal was recorded 6 times in both before and after stage. we do this as current recordings only have the date stored in them, so we convert dates to session number of recording for a given pair and stage, so we can club vocalizations from a given session together later. as vocalization per session become a sample of an indiviuals vocalixations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── Load & annotate experimental log ─────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path      # already imported earlier, but harmless to repeat\n",
    "\n",
    "# Folder that holds the raw / modified log files\n",
    "LOG_DIR = BASE_DIR / \"experimental log files\"\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "file_path_log = LOG_DIR / \"experiment_log_orignal.csv\"      # ← relative\n",
    "df_log = pd.read_csv(file_path_log)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 1. Parse filename and sort chronologically\n",
    "# -----------------------------------------------------------------------------\n",
    "df_log[[\"file_prefix\", \"date\", \"time\", \"ID\"]] = (\n",
    "    df_log[\"file name\"].str.split(\"_\", expand=True)\n",
    ")\n",
    "df_log = df_log.sort_values(by=[\"date\", \"time\"])\n",
    "\n",
    "# Combine 'date' and 'time' (time uses '-' instead of ':')\n",
    "df_log[\"Date_Time\"] = pd.to_datetime(\n",
    "    df_log[\"date\"] + \" \" + df_log[\"time\"].str.replace(\"-\", \":\"),\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 2. Add session numbers\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ensure day / month / year are strings\n",
    "for col in (\"day\", \"month\", \"year\"):\n",
    "    df_log[col] = df_log[col].astype(str)\n",
    "\n",
    "df_log[\"date\"] = pd.to_datetime(\n",
    "    df_log[\"year\"] + \"-\" + df_log[\"month\"] + \"-\" + df_log[\"day\"]\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Unique identifier per pair-date\n",
    "df_log[\"monkey_pair_date\"] = (\n",
    "    df_log[\"ID_left\"] + \"-\" + df_log[\"ID_right\"] + \"-\" + df_log[\"date\"]\n",
    ")\n",
    "\n",
    "# Build session numbers per (pair, period)\n",
    "unique_combos = (\n",
    "    df_log.groupby([\"ID_left\", \"ID_right\", \"period\", \"date\"]).first().reset_index()\n",
    ")\n",
    "\n",
    "session_numbers = {}\n",
    "for _, grp in unique_combos.groupby([\"ID_left\", \"ID_right\", \"period\"]):\n",
    "    grp = grp.sort_values(\"date\")\n",
    "    grp[\"session_number\"] = range(1, len(grp) + 1)\n",
    "    session_numbers.update(\n",
    "        dict(zip(grp[\"monkey_pair_date\"], grp[\"session_number\"]))\n",
    "    )\n",
    "\n",
    "df_log[\"session_number\"] = df_log[\"monkey_pair_date\"].map(session_numbers)\n",
    "df_log.drop(columns=[\"monkey_pair_date\"], inplace=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 3. Save modified log\n",
    "# -----------------------------------------------------------------------------\n",
    "df_log.to_csv(LOG_DIR / \"experiment_log_modified.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now first process the sequnce data, which we extract from the selection tables of the sequnce recordings. we clean and correct any typographical erros made during annoations, modity the element name slighty to only retain main element category name and not positonal or element repeat information. and extract the meta data from the sequnces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nakul\\AppData\\Local\\Temp\\ipykernel_22772\\3135421999.py:104: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  merged_df[seq_cols] = merged_df[seq_cols].applymap(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sequence dataframe shape: (1619, 33)\n",
      "Saved → seqeunce data\\Processed_seq_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── Build sequence dataframe from labelled-sequence .txt files ───────────────\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path                           # already imported earlier\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 1. Folder locations (relative to BASE_DIR)\n",
    "# -----------------------------------------------------------------------------\n",
    "LABELLED_SEQ_DIR   = BASE_DIR / \"labelled sequnces\"\n",
    "PHASE_LABEL_DIR    = BASE_DIR / \"phase change labels\" / \"proccesed_labels\"\n",
    "OUTPUT_SEQ_DIR     = BASE_DIR / \"seqeunce data\"\n",
    "LOG_DIR            = BASE_DIR / \"experimental log files\"\n",
    "\n",
    "for p in (OUTPUT_SEQ_DIR,):\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 2. Helper: read & sort a single sequence-label file\n",
    "# -----------------------------------------------------------------------------\n",
    "def process_text_file(file_path: Path):\n",
    "    df = pd.read_csv(file_path, delimiter=\"\\t\", header=None, names=[\"col1\", \"col2\", \"col3\"])\n",
    "    df = df.sort_values(\"col1\")\n",
    "    return df[\"col3\"].tolist()                     # last column as list\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 3. Build matrix M across all .txt files\n",
    "# -----------------------------------------------------------------------------\n",
    "matrix_M = []\n",
    "for txt_path in LABELLED_SEQ_DIR.glob(\"*.txt\"):\n",
    "    last_col = process_text_file(txt_path)\n",
    "    matrix_M.append([txt_path.name] + last_col)\n",
    "\n",
    "df_M = pd.DataFrame(matrix_M)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 4. Parse filename, attach Date_Time\n",
    "# -----------------------------------------------------------------------------\n",
    "df_M[\n",
    "    [\"stage\", \"type\", \"date\", \"time\", \"recorderID\", \"labelled\", \"focal ID\", \"context\", \"number\"]\n",
    "] = df_M[0].str.split(\"_\", expand=True)\n",
    "\n",
    "df_M[\"Date_Time\"] = pd.to_datetime(df_M[\"date\"] + \" \" + df_M[\"time\"], format=\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 5. Merge with experimental log\n",
    "# -----------------------------------------------------------------------------\n",
    "log_path = LOG_DIR / \"experiment_log_modified.csv\"\n",
    "df_log   = pd.read_csv(log_path)\n",
    "df_log[\"Date_Time\"] = pd.to_datetime(\n",
    "    df_log[\"date\"] + \" \" + df_log[\"time\"].str.replace(\"-\", \":\"),\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(df_M, df_log, how=\"left\", on=\"Date_Time\")\n",
    "\n",
    "# Add conspecific_ID\n",
    "merged_df[\"conspecific_ID\"] = merged_df.apply(\n",
    "    lambda r: r[\"ID_right\"] if r[\"focal ID\"] == r[\"ID_left\"] else r[\"ID_left\"], axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 6. Clean / filter columns exactly as in original code\n",
    "# -----------------------------------------------------------------------------\n",
    "cols_to_drop = [\n",
    "    \"recorderID\", \"labelled\", \"context\", \"number\", \"file name\", \"file type\", \"equipment\",\n",
    "    \"hab_acc\", \"period\", \"ID_left\", \"ID_right\", \"day\", \"month\", \"year\", \"comments\",\n",
    "    \"Unnamed: 12\", \"file_prefix\", \"date_y\", \"time_y\", \"ID\"\n",
    "]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop).dropna(subset=[\"conspecific_ID\"])\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 7. Sequence numbering\n",
    "# -----------------------------------------------------------------------------\n",
    "merged_df = merged_df.sort_values(by=[\"date_x\", \"focal ID\", \"conspecific_ID\"]).reset_index(drop=True)\n",
    "seq_counter, last_keys = 0, (None, None, None)\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    key = (row[\"date_x\"], row[\"focal ID\"], row[\"conspecific_ID\"])\n",
    "    if key != last_keys:\n",
    "        seq_counter = 1\n",
    "        last_keys = key\n",
    "    else:\n",
    "        seq_counter += 1\n",
    "    merged_df.at[idx, \"seq_num\"] = seq_counter\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 8. Original string replacements & sequence processing\n",
    "# -----------------------------------------------------------------------------\n",
    "merged_df = merged_df.replace({\"Nougati\": \"Nougatti\", \"Olympia \": \"Olympia\"})\n",
    "merged_df.loc[\n",
    "    (merged_df[\"focal ID\"] == \"Odin\") & (merged_df[\"conspecific_ID\"] == \"Wuschel\"),\n",
    "    \"conspecific_ID\"\n",
    "] = \"Nougatti\"\n",
    "\n",
    "rep1 = {\"0\": \"x0\", \"x00\": \"x0\", \"Olympia_phee\": \"mA1\", \"mb1\": \"mB1\"}\n",
    "seq_cols = merged_df.columns[1:19]              # same slice as before\n",
    "merged_df[seq_cols] = merged_df[seq_cols].replace(rep1)\n",
    "\n",
    "# remove rows with 'xx' / 'yy'\n",
    "merged_df = merged_df[~merged_df[seq_cols].isin([\"xx\", \"yy\"]).any(axis=1)]\n",
    "\n",
    "# keep only 2nd char except for x0/y0\n",
    "merged_df[seq_cols] = merged_df[seq_cols].applymap(\n",
    "    lambda x: x if x in (\"x0\", \"y0\") else (x[1] if isinstance(x, str) and len(x) > 1 else x)\n",
    ")\n",
    "\n",
    "rep2 = dict.fromkeys(list(\"BCDEFG\"), \"A\") | {\"x0\": \"x\", \"y0\": \"y\", \"O\": \"\"}\n",
    "merged_df[seq_cols] = merged_df[seq_cols].replace(rep2)\n",
    "\n",
    "merged_df[\"sequence\"] = merged_df[seq_cols].apply(\n",
    "    lambda r: \"\".join([str(v) for v in r.values if v not in (None, \"nan\")]), axis=1\n",
    ")\n",
    "\n",
    "# true_sequence (remove x / y)\n",
    "merged_df[\"true_sequence\"] = merged_df[\"sequence\"].str.replace(\"[xy]\", \"\", regex=True)\n",
    "\n",
    "# drop sequences of length ≤ 1\n",
    "filtered_df = merged_df[merged_df[\"true_sequence\"].str.len() > 1].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 9. Attach phase info\n",
    "# -----------------------------------------------------------------------------\n",
    "def extract_phase(file_name: str):\n",
    "    file_prefix = \"_\".join(file_name.split(\"_\")[:-4])\n",
    "    row_num = int(file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    txt_path = PHASE_LABEL_DIR / f\"{file_prefix}.txt\"\n",
    "    if txt_path.exists():\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if 0 < row_num <= len(lines):\n",
    "                return lines[row_num - 1].split()[-1]\n",
    "    return None\n",
    "\n",
    "filtered_df[\"phase\"] = filtered_df[0].apply(extract_phase)\n",
    "filtered_df = filtered_df[filtered_df[\"phase\"].isin([\"1\", \"3\"])]\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 10. Partner / non-partner flag\n",
    "# -----------------------------------------------------------------------------\n",
    "partner_map = {\n",
    "    \"Tabor\": \"Lola\", \"Lola\": \"Tabor\",\n",
    "    \"Odin\": \"Nougatti\", \"Nougatti\": \"Odin\",\n",
    "    \"Wuschel\": \"Olympia\", \"Olympia\": \"Wuschel\",\n",
    "}\n",
    "\n",
    "filtered_df[\"paired_status\"] = filtered_df.apply(\n",
    "    lambda r: \"partner\" if partner_map.get(r[\"focal ID\"]) == r[\"conspecific_ID\"] else \"non-partner\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"Filtered sequence dataframe shape:\", filtered_df.shape)\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# 11. Save processed sequence data\n",
    "# -----------------------------------------------------------------------------\n",
    "out_path = OUTPUT_SEQ_DIR / \"Processed_seq_data.csv\"\n",
    "filtered_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved → {out_path.relative_to(BASE_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now pre process and clean spectral data, using the same calls which are presnt in the sequnce data, but now analysis the spectral structure of the same. we store trandtional acoustic metrics, mfcc and thier pca's along with audio for each call seprately for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variance explained (%)\n",
      "Traditional: [45.97 33.45 12.16  4.06  2.24]\n",
      "MFCC:        [34.19 21.91 11.32  6.29  5.41]\n",
      "Traditional features – variance captured by 5 PCs: 97.88%\n",
      "MFCC features        – variance captured by 5 PCs: 79.12%\n",
      "⚠️  after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020_labelled_mC1.wav skipped – c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\selection table labels\\after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020.txt\n",
      "⚠️  after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020_labelled_mC2.wav skipped – c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\selection table labels\\after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020.txt\n",
      "⚠️  after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020_labelled_mC3.wav skipped – c:\\Users\\nakul\\Desktop\\marmoset project\\2025_Work_Re-start_Marmoset\\2025\\selection table labels\\after_phees_2022-07-21_13-48-50_0000035_labelled_Tabor_pheeseq_0000020.txt\n",
      "\n",
      "---- SUMMARY ----\n",
      "Rows total      : 3907\n",
      "Rows w/ audio   : 3904\n",
      "Saved CSV       : spectral data\\Processed_spec_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── Spectral merge  ➜  trad/MFCC PCA  ➜  single-call extraction ─────────────\n",
    "import os, re, numpy as np, pandas as pd, soundfile as sf\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ── 1. Folders & files (relative to BASE_DIR) ────────────────────────────────\n",
    "SPECTRAL_DATA_DIR = BASE_DIR / \"spectral data\"          # ← new location\n",
    "LOG_DIR            = BASE_DIR / \"experimental log files\"\n",
    "PHASE_LABEL_DIR    = BASE_DIR / \"phase change labels\" / \"proccesed_labels\"\n",
    "SEL_TABLE_DIR      = BASE_DIR / \"selection table labels\"\n",
    "SEQUENCE_AUDIO_DIR = BASE_DIR / \"audio recording sequences\"\n",
    "EXTRACT_DIR        = BASE_DIR / \"extracted calls\"\n",
    "\n",
    "\n",
    "file_data   = SPECTRAL_DATA_DIR / \"MFCC_spectral_updated.csv\"\n",
    "file_extra  = SPECTRAL_DATA_DIR / \"MFCC_spectral_a1.csv\"\n",
    "file_log    = LOG_DIR / \"experiment_log_modified.csv\"\n",
    "\n",
    "# ── 2. Load & concatenate raw spectral CSVs ──────────────────────────────────\n",
    "df_data  = pd.read_csv(file_data)\n",
    "df_extra = pd.read_csv(file_extra)\n",
    "df_data  = pd.concat([df_data, df_extra], ignore_index=True)\n",
    "\n",
    "# ── 3. Parse filename → cols + Date_Time ─────────────────────────────────────\n",
    "df_data[[\"stage\",\"type\",\"date\",\"time\",\"recorderID\",\"labelled\",\"focal ID\",\n",
    "         \"context\",\"no\",\"Labelled\",\"Note\",\"number\"]] = df_data[\"filename\"].str.split(\"_\",expand=True)\n",
    "df_data[\"Date_Time\"] = pd.to_datetime(df_data[\"date\"] + \" \" + df_data[\"time\"],\n",
    "                                      format=\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# ── 4. Merge with experimental log ───────────────────────────────────────────\n",
    "df_log = pd.read_csv(file_log)\n",
    "df_log[\"Date_Time\"] = pd.to_datetime(df_log[\"date\"] + \" \" +\n",
    "                                     df_log[\"time\"].str.replace(\"-\" ,\":\"),\n",
    "                                     format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "merged_df = pd.merge(df_data, df_log, how=\"left\", on=\"Date_Time\")\n",
    "merged_df[\"conspecific_ID\"] = merged_df.apply(\n",
    "    lambda r: r[\"ID_right\"] if r[\"focal ID\"] == r[\"ID_left\"] else r[\"ID_left\"], axis=1)\n",
    "\n",
    "# tidy\n",
    "drop_cols = [\"recorderID\",\"labelled\",\"context\",\"number\",\"file name\",\"file type\",\"equipment\",\n",
    "             \"hab_acc\",\"period\",\"ID_left\",\"ID_right\",\"day\",\"month\",\"year\",\"comments\",\n",
    "             \"file_prefix\",\"date_y\",\"time_y\",\"ID\",\"no\",\"Labelled\",\"type\",\"Date_Time\",\"Unnamed: 12\"]\n",
    "merged_df = merged_df.drop(columns=drop_cols).dropna(subset=[\"conspecific_ID\"])\n",
    "merged_df = merged_df.replace({\"Nougati\":\"Nougatti\",\"Olympia \":\"Olympia\"})\n",
    "merged_df.loc[(merged_df[\"focal ID\"]==\"Odin\") & (merged_df[\"conspecific_ID\"]==\"Wuschel\"),\n",
    "              \"conspecific_ID\"] = \"Nougatti\"\n",
    "merged_df[\"Note\"] = merged_df[\"Note\"].str.slice(0,3)\n",
    "\n",
    "# filename fix (rm trailing _NN.wav)\n",
    "merged_df[\"filename\"] = merged_df[\"filename\"].str.replace(r\"_\\d+\\.wav$\", \".wav\", regex=True)\n",
    "\n",
    "# ── 5. Phase filter (1 & 3 only) ─────────────────────────────────────────────\n",
    "def extract_phase(fname:str):\n",
    "    prefix = \"_\".join(fname.split(\"_\")[:-6])\n",
    "    row_num = int(fname.split(\"_\")[-3].split(\".\")[0])\n",
    "    txt = PHASE_LABEL_DIR / f\"{prefix}.txt\"\n",
    "    if txt.exists():\n",
    "        lines = txt.read_text().splitlines()\n",
    "        if 0 < row_num <= len(lines):\n",
    "            return lines[row_num-1].split()[-1]\n",
    "    return None\n",
    "\n",
    "merged_df[\"phase\"] = merged_df[\"filename\"].apply(extract_phase)\n",
    "merged_df = merged_df[merged_df[\"phase\"].isin([\"1\",\"3\"])]\n",
    "\n",
    "# ── 6. Partner / non-partner flag ────────────────────────────────────────────\n",
    "partner_map = {\"Tabor\":\"Lola\",\"Lola\":\"Tabor\",\n",
    "               \"Odin\":\"Nougatti\",\"Nougatti\":\"Odin\",\n",
    "               \"Wuschel\":\"Olympia\",\"Olympia\":\"Wuschel\"}\n",
    "merged_df[\"paired_status\"] = merged_df.apply(\n",
    "    lambda r: \"partner\" if partner_map.get(r[\"focal ID\"])==r[\"conspecific_ID\"] else \"non-partner\",\n",
    "    axis=1)\n",
    "\n",
    "# ── 7. Z-score + PCA (trad & MFCC blocks) ────────────────────────────────────\n",
    "trad_cols = [\"Dur 90% (s)\",\"Dur 50% (s)\",\"Center Freq (Hz)\",\"Freq 5% (Hz)\",\n",
    "             \"Freq 25% (Hz)\",\"Freq 75% (Hz)\",\"Freq 95% (Hz)\",\"BW 50% (Hz)\",\n",
    "             \"BW 90% (Hz)\",\"Avg Entropy (bits)\",\"Agg Entropy (bits)\"]\n",
    "mfcc_cols = [str(i) for i in range(1,133)]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Traditional metrics  ➜  PCA\n",
    "# ------------------------------------------------------------------\n",
    "scaler_trad = StandardScaler()\n",
    "trad_scaled = scaler_trad.fit_transform(merged_df[trad_cols])\n",
    "\n",
    "pca_trad    = PCA(n_components=5, random_state=42)\n",
    "trad_scores = pca_trad.fit_transform(trad_scaled)\n",
    "\n",
    "# Save scores back to the DF (as you already did)\n",
    "for i in range(5):\n",
    "    merged_df[f\"trad_PC{i+1}\"] = trad_scores[:, i]\n",
    "\n",
    "trad_var = pca_trad.explained_variance_ratio_ * 100   # % variance\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MFCC metrics  ➜  PCA\n",
    "# ------------------------------------------------------------------\n",
    "scaler_mfcc = StandardScaler()\n",
    "mfcc_scaled = scaler_mfcc.fit_transform(merged_df[mfcc_cols])\n",
    "\n",
    "pca_mfcc    = PCA(n_components=5, random_state=42)\n",
    "mfcc_scores = pca_mfcc.fit_transform(mfcc_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    merged_df[f\"mfcc_PC{i+1}\"] = mfcc_scores[:, i]\n",
    "\n",
    "mfcc_var = pca_mfcc.explained_variance_ratio_ * 100   # % variance\n",
    "\n",
    "print(\"\\nVariance explained (%)\")\n",
    "print(\"Traditional:\", np.round(trad_var, 2))\n",
    "print(\"MFCC:       \", np.round(mfcc_var, 2))\n",
    "\n",
    "# variance each PC explains (%)\n",
    "trad_var = pca_trad.explained_variance_ratio_ * 100\n",
    "mfcc_var = pca_mfcc.explained_variance_ratio_ * 100\n",
    "\n",
    "# ── combined variance (sum of those 5 PCs) ─────────────────────\n",
    "total_var_trad = trad_var.sum()\n",
    "total_var_mfcc = mfcc_var.sum()\n",
    "\n",
    "print(f\"Traditional features – variance captured by 5 PCs: {total_var_trad:.2f}%\")\n",
    "print(f\"MFCC features        – variance captured by 5 PCs: {total_var_mfcc:.2f}%\")\n",
    "\n",
    "\n",
    "# ── 8. Extract single-call audio & attach path ───────────────────────────────\n",
    "def extract_single_call(row):\n",
    "    full_fname = row[\"filename\"]\n",
    "    try:\n",
    "        seq_part, label_part = full_fname.rsplit(\"_labelled_\",1)\n",
    "        seq_base   = seq_part\n",
    "        call_label = label_part.replace(\".wav\",\"\")\n",
    "\n",
    "        sel_tab = SEL_TABLE_DIR / f\"{seq_base}.txt\"\n",
    "        if not sel_tab.exists(): raise FileNotFoundError(sel_tab)\n",
    "\n",
    "        start_s = end_s = None\n",
    "        for ln in sel_tab.read_text().splitlines():\n",
    "            if not ln.strip(): continue\n",
    "            s,e,lbl = ln.split()[:3]\n",
    "            if lbl == call_label:\n",
    "                start_s, end_s = float(s), float(e)\n",
    "                break\n",
    "        if start_s is None: raise ValueError(f\"label {call_label} not found\")\n",
    "\n",
    "        seq_wav = SEQUENCE_AUDIO_DIR / f\"{seq_base}_labelled.wav\"\n",
    "        if not seq_wav.exists(): raise FileNotFoundError(seq_wav)\n",
    "\n",
    "        audio, sr = sf.read(seq_wav, always_2d=False)\n",
    "        segment = audio[int(start_s*sr): int(end_s*sr)]\n",
    "\n",
    "        out_path = EXTRACT_DIR / full_fname\n",
    "        sf.write(out_path, segment, sr)\n",
    "        return str(out_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  {full_fname} skipped – {e}\")\n",
    "        return pd.NA\n",
    "\n",
    "merged_df[\"call_audio_path\"] = merged_df.apply(extract_single_call, axis=1)\n",
    "clean_df = merged_df.dropna(subset=[\"call_audio_path\"]).reset_index(drop=True)\n",
    "\n",
    "# ── 9. Save ONLY the with-paths CSV to ‘spectral data/’ ───────────────────────\n",
    "out_csv = SPECTRAL_DATA_DIR / \"Processed_spec_data.csv\"\n",
    "clean_df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\n---- SUMMARY ----\")\n",
    "print(\"Rows total      :\", len(merged_df))\n",
    "print(\"Rows w/ audio   :\", len(clean_df))\n",
    "print(\"Saved CSV       :\", out_csv.relative_to(BASE_DIR))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
